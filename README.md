# 量潮大数据示例项目

# CSV 工具箱

本项目旨在提供一系列实用的小工具，帮助用户处理和分析 CSV 文件。

## 功能列表

### 1. CSV 文件字符集检查与原地UTF-8转换器

*   **功能描述**：
    1.  使用 `chardet` 检测 CSV 文件的初始字符集编码和置信度。
    2.  **智能解码尝试**：特别是对于 `chardet` 检测为中文编码（如GB2312）的情况，脚本会优先尝试使用更全面的编码（如GB18030, GBK）以 `errors='strict'` 模式读取文件，以最大程度保证中文字符的正确解析。如果所有严格模式尝试失败，会回退到使用 `chardet` 的初始检测值配合 `errors='replace'`（可能导致字符替换）。
    3.  如果文件内容成功读取且原始编码不是 UTF-8，则**直接修改原始文件**，将其内容转换为 UTF-8 编码。
    4.  **自动备份**：在修改原始文件前，默认会自动创建一个备份文件（例如 `filename.csv.bak`）。
*   **依赖库**：`chardet`, `shutil` (shutil 是 Python 内置库，无需额外安装)
*   **文件名**：`csv_charset_checker.py`

*   **警告：此脚本会直接修改你的原始文件！**
    *   虽然脚本包含自动备份功能（默认开启）和更智能的解码尝试，但直接修改文件本身存在数据损坏的风险，尤其是在编码检测最终不准确或程序意外中断的情况下。
    *   **强烈建议在使用此脚本处理重要文件前，先手动备份你的文件，或在测试文件上充分验证脚本行为。**

*   **主要使用方法：直接在脚本中配置路径**

    1.  **打开脚本**：用文本编辑器打开 `csv_charset_checker.py` 文件。
    2.  **找到用户配置区**：在文件末尾的 `if __name__ == "__main__":` 代码块中，你会看到一个标记为 `--- 用户配置区 ---` 的部分。
    3.  **配置路径**：修改 `target_paths_input` 变量，填入你要处理的一个或多个 CSV 文件路径。
        *   **单个文件示例**：`target_paths_input = r"D:\数据\我的文件.csv"`
        *   **多个文件示例**：
            ```python
            target_paths_input = [
                r"C:\项目甲\数据1.csv",
                r"D:\共享文件夹\数据2.csv",
            ]
            ```
        *   **路径格式提示**：Windows 路径建议使用 `r"C:\path\to\file.csv"` 或 `"C:/path/to/file.csv"` 格式。
    4.  **配置参数（重要）**：
        *   `sample_size_to_use`: 用于检测编码的样本字节数 (默认 10240)。
        *   `CREATE_BACKUP_FILES = True`: **是否创建备份文件**。默认为 `True`（强烈建议）。如果设置为 `False`，脚本将不会创建备份而直接修改原始文件，风险极高。
        *   `minimum_confidence_for_conversion_hint`: 编码检测置信度的提示阈值 (默认 0.7)。
    5.  **保存并运行脚本**：保存修改后，在命令行中直接运行：
        ```bash
        python csv_charset_checker.py
        ```
*   **工作流程**：
    1.  脚本会逐个处理您在 `target_paths_input` 中指定的每个文件。
    2.  对每个文件，首先使用 `chardet` 库初步检测其字符集编码。
    3.  **智能解码与内容读取**：
        *   脚本会根据 `chardet` 的初步检测结果，建立一个尝试解码的编码列表。例如，如果初步检测为 `GB2312`，列表可能优先包含 `GB18030`、`GBK`，然后是 `GB2312`。
        *   脚本会依次使用列表中的编码，以 `errors='strict'`（严格）模式尝试完整读取文件内容。此模式能确保不丢失字符信息，但如果编码不匹配则会失败。
        *   第一个成功以 `errors='strict'` 模式读取文件内容的编码被认为是实际的原始编码。
        *   **如果所有 `errors='strict'` 尝试都失败**，脚本会进行最后一次尝试：使用 `chardet` 最初检测到的编码配合 `errors='replace'` 模式读取。此模式会用特殊替换符替换无法解码的字节，**可能导致部分数据丢失或变成乱码**，但能确保至少能读出大部分内容。脚本会对此进行提示。
        *   如果最终未能读取文件内容，则无法进行转换。
    4.  **自动备份与原地转换** (仅当文件内容成功读取且原始编码非UTF-8时进行)：
        *   如果 `CREATE_BACKUP_FILES` 为 `True` (默认)：脚本会为原始文件创建备份 (例如 `filename.csv.bak`)。
        *   脚本将使用成功读取到的内容，以 UTF-8 编码**直接写回到原始文件路径**，覆盖原有内容。
        *   如果文件本身已经是 UTF-8 编码（根据其原始正确读取的编码判断或 `chardet` 初检），则不进行修改。
    5.  **结果输出**：脚本会打印详细的处理日志，包括 `chardet` 初检结果、尝试的解码顺序、实际用于读取的编码、备份创建情况以及转换是否成功。

*   **重要提示与风险管理**：
    *   **数据安全第一**：再次强调，由于脚本直接修改原始文件，请务必谨慎操作。**始终优先考虑手动备份重要数据，或利用脚本的自动备份功能（保持 `CREATE_BACKUP_FILES = True`）。**
    *   **编码检测准确性**：转换的质量高度依赖于原始编码检测的准确性。如果原始编码判断错误，即使有备份，恢复后的文件内容也可能不是你最初期望的。若转换后的文件内容不正确，请立即从 `.bak` 文件恢复，并考虑使用其他方法或工具确认原始编码。
    *   **错误与恢复**：如果转换过程中发生错误（如解码失败），脚本会给出提示。此时，你的原始文件可能已被部分修改或处于不一致状态。请立即检查备份文件（如果已创建）并用其恢复原始数据。

*   **输出示例 (可能包含更详细的解码尝试日志)**：
    ```
    --- 开始字符集检测与原地UTF-8转换 ---
    
    [文件 1/1] 开始处理: D:\学习资料\大三下\大数据应用实践\data\data\categories.csv
    文件: D:\学习资料\大三下\大数据应用实践\data\data\categories.csv
    检测到的字符集: GB2312 (置信度: 0.99)
    文件 'D:\学习资料\大三下\大数据应用实践\data\data\categories.csv': chardet检测为 'GB2312'. 尝试解码顺序: ['gb18030', 'gbk', 'GB2312']
      尝试使用编码 'gb18030' (strict模式) 读取...
      使用编码 'gb18030' 解码失败 (UnicodeDecodeError)。
      尝试使用编码 'gbk' (strict模式) 读取...
      成功使用编码 'gbk' 读取文件内容。
    已为 'D:\学习资料\大三下\大数据应用实践\data\data\categories.csv' 创建备份: 'D:\学习资料\大三下\大数据应用实践\data\data\categories.csv.bak'
    准备将文件 'D:\学习资料\大三下\大数据应用实践\data\data\categories.csv' (实际读取编码: gbk) 直接写入为 UTF-8...
    成功！文件 'D:\学习资料\大三下\大数据应用实践\data\data\categories.csv' 已直接转换为 UTF-8 编码。
    原始文件备份在: 'D:\学习资料\大三下\大数据应用实践\data\data\categories.csv.bak'
    
    --- 所有文件处理完成 ---
    
    成功转换并原地修改为UTF-8的文件列表:
      - D:\学习资料\大三下\大数据应用实践\data\data\categories.csv
        (原始文件备份通常在此文件同目录下，以 .bak 或 .bak.n 结尾)
    
    --- 检测与原地转换结束 ---
    ```

*   **备选使用方法：通过命令行参数运行 (高级)**
    (此功能的相关代码在脚本中已被注释，如需使用请先取消注释)

### 2. CSV 文件首行移除工具

*   **功能描述**：
    1.  遍历指定目录下的所有 `.csv` 文件。
    2.  对每个CSV文件，**直接修改原始文件**以删除其第一行内容（通常是表头）。
    3.  **保留原始编码**：脚本会先通过 `chardet` 和备选列表尝试可靠地检测每个文件的原始编码，并在删除首行后，以相同的原始编码保存文件，以避免内容（尤其是中文）损坏。
    4.  **自动备份**：在修改原始文件前，默认会自动创建一个备份文件（例如 `filename.csv.bak`）。
*   **依赖库**：`chardet`, `shutil`, `tempfile` (均为Python内置或通过pip安装)
*   **文件名**：`csv_header_remover.py`

*   **警告：此脚本会直接修改你的原始文件！**
    *   虽然脚本包含自动备份功能（默认开启），但直接修改文件本身存在数据损坏的风险，尤其是在编码检测不完全准确或程序意外中断的情况下。
    *   **强烈建议在使用此脚本处理重要文件前，先手动备份你的文件，或在测试文件上充分验证脚本行为。**

*   **使用方法**：
    1.  **打开脚本**：用文本编辑器打开 `csv_header_remover.py` 文件。
    2.  **找到用户配置区**：在文件顶部，你会找到一个标记为 `--- 用户配置区 ---` 的部分。
    3.  **配置目标目录**：修改 `TARGET_DIRECTORY` 变量，填入包含你想要处理的CSV文件的文件夹的完整路径。例如：
        *   Windows: `TARGET_DIRECTORY = r"D:\\My CSVs"`
        *   Linux/macOS: `TARGET_DIRECTORY = "/home/user/my_csvs"`
    4.  **配置备份选项** (可选但推荐)：
        *   `CREATE_BACKUP_FILES = True`: 默认为 `True`，会在修改前创建 `.bak` 备份。如果设为 `False`，则不创建备份，风险较高。
    5.  **(可选) 调整编码检测参数**：
        *   `ENCODING_SAMPLE_SIZE`: 检测编码时读取的样本字节数。
        *   `FALLBACK_ENCODINGS`: 当 `chardet` 初始检测不明确或验证失败时，会尝试此列表中的编码。
    6.  **保存并运行脚本**：保存修改后，在你的命令行或终端中，导航到脚本所在的目录，然后运行：
        ```bash
        python csv_header_remover.py
        ```
        脚本将开始处理指定目录下的所有CSV文件。

*   **工作流程**：
    1.  脚本会遍历 `TARGET_DIRECTORY` 中的所有文件。
    2.  对于每个以 `.csv` 结尾的文件：
        *   首先尝试使用 `robust_detect_encoding` 函数确定其原始编码。该函数结合 `chardet` 和一个预定义的 `FALLBACK_ENCODINGS` 列表，力求准确检测。
        *   如果未能可靠检测到编码，为防止数据损坏，将跳过此文件。
        *   如果 `CREATE_BACKUP_FILES` 为 `True`，则创建原始文件的备份（如 `filename.csv.bak`）。
        *   使用检测到的原始编码，读取原始CSV文件，跳过第一行，将其余所有行写入一个安全的临时文件。
        *   用此临时文件替换原始文件，从而完成首行删除操作。
        *   打印每个文件的处理状态。
    3.  处理完成后，会输出一个总结报告。

*   **输出示例**：
    ```
    --- 开始处理目录: D:\TestCSVs ---
    将为每个修改的文件创建备份 (后缀 .bak)。
    开始处理文件: D:\TestCSVs\data1_gbk.csv
      Chardet检测到编码: GBK (置信度: 0.99), 验证通过。
      将使用编码 'GBK' (读取选项: strict) 和 'GBK' (写入选项: strict) 进行操作。
      已创建备份: 'D:\TestCSVs\data1_gbk.csv.bak'
      已跳过第一行。
      成功删除第一行并覆盖原文件: 'D:\TestCSVs\data1_gbk.csv'
    ------------------------------
    开始处理文件: D:\TestCSVs\data2_utf8.csv
      Chardet检测到编码: utf-8 (置信度: 1.00), 验证通过。
      将使用编码 'utf-8' (读取选项: strict) 和 'utf-8' (写入选项: strict) 进行操作。
      已创建备份: 'D:\TestCSVs\data2_utf8.csv.bak'
      已跳过第一行。
      成功删除第一行并覆盖原文件: 'D:\TestCSVs\data2_utf8.csv'
    ------------------------------
    
    --- 处理完毕 ---
    总共扫描到CSV文件数: 2
    成功删除第一行的文件数: 2
    所有扫描到的CSV文件均已成功处理。
    脚本执行结束。
    ```

## 未来可能的改进

*   **通用CSV工具**：
    *   提供选项，允许用户在转换前预览几行内容。
    *   增加更完善的错误恢复建议或选项。
    *   允许用户自定义备份文件的位置或命名规则。
*   **首行移除工具**：
    *   允许用户指定要删除的行数（不止第一行）。
    *   增加按文件名模式匹配（例如，只处理以 `report_` 开头的文件）。

### 3. 每日分类商品价格指数计算器 (ClickHouse)

*   **功能描述**：
    1.  连接到用户指定的 ClickHouse 数据库。
    2.  从配置的商品价格表、商品信息表和商品分类表中提取数据。
    3.  计算每日、每个商品分类按销量加权的平均价格。
    4.  输出结果包括：日期、分类名称、加权平均价格、该分类当日总销量、该分类当日售出商品种数。
    5.  提供将查询结果保存为 CSV 文件的选项 (代码中默认注释，可取消注释以启用)。
*   **依赖库**：`clickhouse-connect` (请确保已通过 `pip install clickhouse-connect` 安装，或通过 `pip install -r requirements.txt` 安装项目所有依赖)。
*   **文件名**：`cpi_calculator.py`

*   **使用方法**：
    1.  **安装依赖**：
        ```bash
        pip install -r requirements.txt
        ```
    2.  **配置脚本**：打开 `cpi_calculator.py` 文件，找到顶部的 `--- 用户配置区 ---`。
    3.  **修改连接参数**：根据您的 ClickHouse 环境，修改以下变量：
        *   `CLICKHOUSE_HOST`: ClickHouse 服务器地址 (例如: '127.0.0.1')
        *   `CLICKHOUSE_PORT`: ClickHouse HTTP(S) 端口 (例如: 8123 for http, 8443 for https)
        *   `CLICKHOUSE_USER`: 用户名
        *   `CLICKHOUSE_PASSWORD`: 密码
        *   `CLICKHOUSE_DATABASE`: 数据库名称
    4.  **修改表名和列名** (如果需要)：
        *   `TABLE_CATEGORIES`, `TABLE_PRODUCTS`, `TABLE_PRICES`: 这三个变量定义了分类表、商品表和价格数据表的名称。
        *   `COL_CATEGORY_ID`, `COL_CATEGORY_NAME` 等变量定义了这些表中关键列的名称。请确保这些名称与您数据库中的实际名称一致。
    5.  **运行脚本**：保存修改后，在命令行或终端中运行：
        ```bash
        python cpi_calculator.py
        ```
        脚本将尝试连接数据库，执行查询，并打印结果摘要。如果启用了CSV保存功能，结果也会写入到 `category_weighted_prices.csv` 文件中。

*   **SQL 查询逻辑简述**：
    脚本中的核心 SQL 查询会执行以下操作：
    *   通过 `JOIN` 关联价格数据表、商品信息表和分类信息表。
    *   使用 `toDate()` 函数确保按天聚合数据。
    *   计算 `SUM(price * sales_volume) / SUM(sales_volume)` 作为加权平均价格。
    *   使用 `WHERE` 子句过滤掉销量为零或价格为空的记录，以确保计算的准确性。
    *   使用 `GROUP BY` 按分类名称和日期对结果进行分组。
    *   使用 `ORDER BY` 对结果进行排序。

*   **注意事项**：
    *   确保 ClickHouse 服务正在运行，并且网络连接通畅。
    *   提供的用户凭证需要有权限读取相关的表。
    *   如果您的 ClickHouse 使用 HTTPS/TLS 加密连接，请在 `get_clickhouse_client` 函数中取消注释并适当配置 `secure=True` 和 `verify` 参数。
    *   脚本中假设的日期列 (`event_date`) 可以被 `toDate()` 函数正确转换为日期。如果您的日期格式特殊，可能需要调整查询中的日期处理方式。
